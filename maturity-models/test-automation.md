# テスト自動化（Test Automation）成熟度モデル

## ケイパビリティ概要

テスト自動化は、ソフトウェアデリバリーライフサイクル全体を通じて、変更の影響に関する高速フィードバックを得るための重要なプラクティスです。品質をソフトウェアに組み込むための鍵は、変更の影響について高速フィードバックを得ることです。

**カテゴリ**: Fast Feedback（高速フィードバック）
**DORA分類**: コアケイパビリティ

---

## テストの種類（DORA定義）

### Unit Tests（単体テスト）
- 単一のメソッド、クラス、または関数を独立してテスト
- **外部依存のみモック/スタブで置換**（DB、外部API、ファイルシステム等）
- **内部のコラボレーターは実物を使用**（古典学派アプローチ）
- テストピラミッドの最下層（最も多い）
- **評価指標**: コードカバレッジ（行カバレッジまたは分岐カバレッジ）

### Acceptance Tests（受入テスト）
Acceptance Testsは、実行中のアプリまたはサービスをテストし、高レベルの機能が設計通りに動作することを保証します。デプロイメントパイプラインでは、以下のフェーズに分かれます:

#### Component/Integration Tests（コンポーネント/統合テスト）
- 複数の**内部コンポーネント/モジュール**間の連携をテスト
- **内部コンポーネントは実物を使用**
- **外部システム（DB、外部API、MQ等）はすべてモック/スタブで置換**
- **内部の**モジュール間インターフェースの正確性を検証
- **評価指標**: 自動化率 = 自動テスト済みユースケース数 / 総ユースケース数
  - **ユースケース粒度**: 内部統合ユースケース（機能単位）
  - **定義**: 複数の内部コンポーネント/モジュールが連携して実現する機能
  - **例**: 注文処理（Order Service + Inventory Service + Notification Service）、在庫更新（Inventory Service + Warehouse Service）

#### Integration Tests with External Systems（外部システム統合テスト）
- **外部システム（DB、外部API、MQ等）との連携のみ**をテスト
- **内部コンポーネント間の統合は含まない**（それはComponent/Integration Testsで実施）
- **可能な限り実環境に近い環境で実行**（テスト用DB、ローカルAPI等）
- 外部依存との接続を検証
- **評価指標**: 自動化率 = 自動テスト済みユースケース数 / 総ユースケース数
  - **ユースケース粒度**: 外部連携ユースケース（連携単位）
  - **定義**: 外部システムとの1つの連携操作（読み書き、API呼び出し等）
  - **例**: 顧客データをDBに保存、決済APIで課金実行、メール送信サービスにメッセージ送信

#### End-to-End Tests（E2Eテスト）
- システム全体のユースケースをエンドツーエンドでテスト
- 本番に近い統合環境で実行
- ビジネスシナリオの正確性を検証
- テストピラミッドの最上層（最も少ない）
- **評価指標**: 自動化率 = 自動テスト済みユースケース数 / 総ユースケース数
  - **ユースケース粒度**: エンドツーエンドユースケース（ユーザージャーニー）
  - **定義**: ユーザーがシステムを使って達成する一連の業務フロー
  - **例**: 新規会員登録→商品検索→カート追加→購入→注文確認メール受信、請求書作成→承認→支払い処理→完了通知

---

## 評価軸の定義

### 1. 網羅性（Coverage）

**何を測るか**: テストがどれだけ網羅されているか（カバレッジ率・自動化率）

**なぜ重要か**:
- **テストは実行可能な仕様書である** - テストの省略は仕様の省略と同義であり、できる限り避けるべき
- テストされていないコードは、バグが隠れている可能性が高い
- 網羅性が低いと、リグレッション（既存機能の劣化）を検知できない
- 高い網羅性は、安心してリファクタリングできる基盤となる

**評価方法**:
- Unit Tests: コードカバレッジ率（行カバレッジまたは分岐カバレッジ）
- Acceptance Tests: ユースケース自動化率（自動テスト済みユースケース数 / 総ユースケース数）

**具体例**:
- レベル1: Unit Testカバレッジ5%未満、E2Eテストなし
- レベル3: Unit Testカバレッジ60%以上、E2Eユースケース30%自動化
- レベル5: Unit Testカバレッジ75%以上、E2Eユースケース70%自動化

---

### 2. テストの質（Quality）

**何を測るか**: テストの有効性・保守性（アサーションの質、独立性、リグレッション検知能力、テストパターンの活用）

**なぜ重要か**:
- カバレッジが高くても、質の低いテスト（アサーションなし、常にパス）は価値がない
- 実装詳細に依存するテストは、リファクタリングのたびに壊れてメンテナンスコストが高い
- テストパターンを活用しないと、テストコードが重複・複雑化し、保守困難になる

**評価方法**:
- アサーションの有無と品質
- テストの独立性（順序依存しない）
- 実装詳細からの独立性（振る舞いを検証）
- テストパターンの活用度（Builder、POM等）
- ミューテーションテストスコア（レベル5）

**具体例**:
- レベル1: アサーションなし、実装詳細に依存、テストデータ作成が煩雑
- レベル3: 意味のあるアサーション、Test Data Builder活用、Given-When-Then構造
- レベル5: ミューテーションテスト実施、テストパターン組織標準化

---

### 3. 実行品質（Execution）

**何を測るか**: テスト実行の速さ・安定性（実行時間、Flaky Test率）

**なぜ重要か**:
- 遅いテストは、開発者が実行を避けるようになり、フィードバックサイクルが遅くなる
- Flaky Test（不安定なテスト）は、開発者の信頼を損ない、本物のバグを見逃す原因となる
- 高速で安定したテストは、CI/CDパイプラインの基盤となる

**評価方法**:
- 実行時間（Unit: 2-10分、E2E: 15-30分）
- Flaky Test率（不安定なテストの割合）
- 実行環境の安定性

**具体例**:
- レベル1: 測定なし、Flaky Test多数
- レベル3: Unit Test 10分未満、Flaky率2%未満
- レベル5: Unit Test 2分未満、Flaky率0.1%未満

---

### 4. 開発者文化（Culture）

**何を測るか**: 開発者の主体的関与度（開発者主導、TDD実践度）

**なぜ重要か**:
- **DORA核心原則**: 開発者がテストを主体的に作成・保守することが、品質向上の鍵
- QA依存の組織では、フィードバックサイクルが遅く、品質問題が後工程で発見される
- 開発者主導のテスト文化は、コードとテストの一体性を高め、保守性を向上させる

**評価方法**:
- 誰がテストを書いているか（開発者 vs QA）
- TDD実践度（試験的 → 推奨 → 標準 → 組織文化）
- テストなしを完了の定義（DoD）として許容しているか

**具体例**:
- レベル1: 開発者がテストを書いていない、TDDなし
- レベル3: 開発者がテストを作成・保守（DORA原則達成）、TDD推奨
- レベル5: 組織全体で開発者主導が定着、TDD組織文化

---

### 5. 戦略（Strategy）

**何を測るか**: 計画性・継続的改善（文書化、定期的見直し、データドリブン）

**なぜ重要か**:
- 場当たり的なテストでは、重複や抜け漏れが発生する
- 文書化された戦略は、チーム間で一貫性を保ち、新メンバーのオンボーディングを容易にする
- データドリブンな優先度付けは、ROIの高いテストに注力できる

**評価方法**:
- テスト戦略の文書化レベル（暗黙的 → 基本方針 → 詳細定義）
- 定期的な見直しの有無（四半期ごと等）
- データドリブンな優先度付け（バグ履歴、変更頻度等）

**具体例**:
- レベル1: 暗黙的（口頭、場当たり的）
- レベル3: 詳細に定義（役割分担、カバレッジ目標、テストパターン等）
- レベル5: 継続的最適化、テストの価値を定量評価

---

## テストタイプ別の成熟度評価

### Unit Tests（単体テスト）

#### 評価軸1: 網羅性（Coverage）

**レベル1: 初期段階**
- □ コードカバレッジが5%未満
- □ Unit Testがほぼ存在しない、または正常系の一部のみ

**レベル2: 管理段階**
- □ コードカバレッジが20%以上
- □ ビジネスロジックの主要な正常系をテスト（エッジケース・異常系は未カバー）

**レベル3: 定義段階**
- □ コードカバレッジが60%以上
- □ すべての新規コードに対してUnit Testを作成（主要なエッジケース・異常系を含む）

**レベル4: 定量管理段階**
- □ コードカバレッジが70%以上
- □ 変更頻度・バグ履歴・ビジネス影響度に基づいてテスト優先度を決定している

**レベル5: 最適化段階**
- □ コードカバレッジが75%以上
- □ リスクベースのテスト戦略により、重要度の高いコードに対してより高品質なテストを作成している
- □ カバレッジメトリクスをダッシュボードで可視化し、チーム全体で共有している
- □ Mutation Testingを導入し、テストの質を定量的に評価している

---

#### 評価軸2: テストの質（Quality）

**レベル1: 初期段階**
- □ アサーションのないテスト、または常にパスするテストが多数存在
- □ テストが実装詳細に強く依存している（リファクタリングで頻繁に壊れる）
- □ テストデータ作成が煩雑（コンストラクタで全パラメータを毎回指定）

**レベル2: 管理段階**
- □ 基本的なアサーションがある（`assertEquals`等）
- □ 一部のテストは振る舞いを検証しているが、実装詳細に依存するテストも多い
- □ テストデータの重複が多く、保守が困難

**レベル3: 定義段階**
- □ すべてのテストに意味のあるアサーションがある
- □ テストは振る舞いを検証し、実装詳細から独立している
- □ テストが独立して実行可能（順序に依存しない）
- □ Test Data Builder、Object Mother等のパターンでテストデータ作成を簡素化
- □ Given-When-Then等の構造化パターンでテストが整理されている
- □ Acceptance Test等で見つかったバグは必ずUnit Testに追加するルールがある

**レベル4: 定量管理段階**
- □ エッジケース・異常系を包括的にカバーしている
- □ テストコードの保守性が高く、読みやすい
- □ Fixture、Factory等を活用し、テストコードの重複を排除
- □ バグトラッキングシステムと連携し、上位テストのバグは自動的にUnit Test追加が促される

**レベル5: 最適化段階**
- □ ミューテーションテストを定期実行し、テストの有効性を検証している
- □ テストコードレビューで保守性・可読性を確認している
- □ テストパターンが組織的に標準化され、チーム間で共有されている
- □ 不要なテスト削除・重複排除など継続的にリファクタリングしている

---

#### 評価軸3: 実行品質（Execution）

**レベル1: 初期段階**
- □ 実行時間を測定していない
- □ Flaky Testの発生率を測定していない

**レベル2: 管理段階**
- □ 実行時間をCIパイプラインで自動測定し、トレンドを記録している
- □ Flaky Test発生を検知しているが、体系的な対処はしていない

**レベル3: 定義段階**
- □ 実行時間が10分未満を維持している（DORA推奨基準）
- □ 実行時間が基準を超えた場合、改善タスクが作成される
- □ Flaky Test率が2%未満
- □ Flaky Testを検知したら自動でIssueが作成される

**レベル4: 定量管理段階**
- □ 実行時間が5分未満を維持している
- □ 実行時間のトレンド分析を行い、劣化を事前に検知している
- □ Flaky Test率が0.5%未満
- □ Flaky Testの根本原因を分析し、体系的に対処している

**レベル5: 最適化段階**
- □ 全テストの実行時間が2分未満を維持している
- □ Flaky Test率が0.1%未満
- □ 実行時間の遅いテストを継続的に最適化している（並列化、テストデータ最適化等）
- □ テスト実行のパフォーマンスメトリクスをダッシュボードで可視化している

---

#### 評価軸4: 開発者文化（Culture）

**レベル1: 初期段階**
- □ 開発者がテストを書いていない
- □ TDDの実践なし

**レベル2: 管理段階**
- □ 一部の開発者がテストを書いている
- □ 一部の開発者がTDDを試験的に実施している

**レベル3: 定義段階**
- □ 開発者がテストを作成・保守している（DORA原則）
- □ TDDが推奨プラクティスとして文書化されている

**レベル4: 定量管理段階**
- □ 開発者主導のテスト文化が定着している
- □ TDDが標準プラクティスとして定着（大半の新規コードでTDD）

**レベル5: 最適化段階**
- □ 開発者主導のテスト文化が組織全体に定着している
- □ TDDが組織文化として定着（すべての新規コードでTDD）

---

#### 評価軸5: 戦略（Strategy）

**レベル1: 初期段階**
- □ テスト戦略が暗黙的（口頭、場当たり的、Unit/IT/E2Eの境界が曖昧）
- □ 上位テストで見つかったバグをUnit Testに追加する習慣がない

**レベル2: 管理段階**
- □ テスト戦略が文書化されている（基本方針レベル：「重要な部分はテストする」等）

**レベル3: 定義段階**
- □ テスト戦略が詳細に定義されている（Unit/IT/E2Eの役割分担、外部依存のみモック、カバレッジ目標等）

**レベル4: 定量管理段階**
- □ テスト戦略を定期的に見直している（四半期ごと等）

**レベル5: 最適化段階**
- □ テストの価値を定量評価し、継続的に最適化している

### Component/Integration Tests（コンポーネント/統合テスト）

#### 評価軸1: 網羅性（Coverage）

**評価指標**: 自動化率 = 自動テスト済みユースケース数 / 総ユースケース数
- **ユースケース粒度**: 内部統合ユースケース（機能単位） - 複数の内部コンポーネント/モジュールが連携して実現する機能

**レベル1: 初期段階**
- □ 自動化された統合テストが存在しない
- □ 内部統合ユースケースが定義されていない

**レベル2: 管理段階**
- □ コアビジネスフローの内部統合ユースケースを一部自動テストしている（自動化率10%以上）
- □ 最も重要なユースケースのみカバー（例: 注文処理、在庫更新）

**レベル3: 定義段階**
- □ 要件定義またはユーザーストーリーで内部統合ユースケースが定義されている
- □ 定義済みユースケースの50%以上を自動テストしている

**レベル4: 定量管理段階**
- □ 定義済みユースケースの75%以上を自動テストしている

**レベル5: 最適化段階**
- □ 定義済みユースケースの90%以上を自動テストしている
- □ リスクベースのテスト戦略により、変更頻度・ビジネス影響度の高い統合に対してより高品質なテストを作成している
- □ 統合テストメトリクスをダッシュボードで可視化し、チーム全体で共有している
- □ Consumer-Driven Contracts（CDC）を導入し、契約ベースのテストを実現している

---

#### 評価軸2: テストの質（Quality）

**レベル1: 初期段階**
- □ 統合テストが存在しない、または手動テストのみ
- □ テストなしを完了の定義としている
- □ テストデータのセットアップが複雑で、各テストで重複している

**レベル2: 管理段階**
- □ 正常系のみテスト（異常系は未カバー）
- □ テストが実装詳細に依存している
- □ モックやスタブを使用しているが、一貫性がない

**レベル3: 定義段階**
- □ 基本的な異常系（Null応答、例外処理等）をテストしている
- □ テストが独立して実行可能
- □ Test Data Builder等でテストデータ作成を簡素化
- □ Arrange-Act-Assert（AAA）パターンでテストが整理されている

**レベル4: 定量管理段階**
- □ 主要な異常系（Null応答、例外処理、エラーハンドリング）を包括的にテストしている
- □ 主要な内部コンポーネント間で契約テスト（Consumer-Driven Contracts）を実施している
- □ Test Fixture、Shared Fixture等でセットアップを共通化
- □ Fake Object、Test Doubleパターンが統一されている

**レベル5: 最適化段階**
- □ すべての異常系とエッジケースをテストしている
- □ すべての内部コンポーネント間でCDC（Consumer-Driven Contracts）が標準化されている
- □ 契約のバージョニングと互換性管理を実施している
- □ テストパターンが組織的に標準化され、チーム間で共有されている
- □ 不要なテスト削除・重複排除など継続的に最適化している

---

#### 評価軸3: 実行品質（Execution）

**レベル1: 初期段階**
- □ 実行時間を測定していない
- □ Flaky Testの発生率を測定していない

**レベル2: 管理段階**
- □ 実行時間をCIパイプラインで自動測定し、トレンドを記録している
- □ Flaky Test発生を検知しているが、体系的な対処はしていない

**レベル3: 定義段階**
- □ 実行時間が15分未満を維持している
- □ 実行時間が基準を超えた場合、改善タスクが作成される
- □ Flaky Test率が2%未満

**レベル4: 定量管理段階**
- □ 実行時間が10分未満を維持している
- □ 実行時間のトレンド分析を行い、劣化を事前に検知している
- □ Flaky Test率が0.5%未満

**レベル5: 最適化段階**
- □ 実行時間が5分未満を維持している
- □ Flaky Test率が0.1%未満
- □ 実行時間の遅いテストを継続的に最適化している（並列化、テストデータ最適化等）

---

#### 評価軸4: 開発者文化（Culture）

**レベル1: 初期段階**
- □ 開発者がテストを書いていない

**レベル2: 管理段階**
- □ 一部の開発者がテストを書いている

**レベル3: 定義段階**
- □ 開発者がテストを作成・保守している（DORA原則）

**レベル4: 定量管理段階**
- □ 開発者主導のテスト文化が定着している

**レベル5: 最適化段階**
- □ 開発者主導のテスト文化が組織全体に定着している
- □ テストなしの完了の定義（DoD）が組織文化として許容されない

---

#### 評価軸5: 戦略（Strategy）

**レベル1: 初期段階**
- □ テスト戦略が暗黙的（口頭、場当たり的）

**レベル2: 管理段階**
- □ テスト戦略が文書化されている（基本方針レベル：「ビジネスクリティカルな内部連携のみテスト」等）
- □ ビジネスクリティカルな機能のみテスト合格が完了の定義（DoD）の条件

**レベル3: 定義段階**
- □ テスト戦略が詳細に定義されている（内部統合ポイント一覧、各ポイントのテスト方針、異常系カバレッジ等）
- □ 統合テスト合格が完了の定義（DoD）の前提条件として標準化されている

**レベル4: 定量管理段階**
- □ 契約テストの戦略が文書化されている（プロバイダー・コンシューマー双方のテスト方針）
- □ すべての統合テスト + 契約テスト合格が完了の定義（DoD）の必須条件

**レベル5: 最適化段階**
- □ テスト戦略を定期的に見直している（四半期ごと等）
- □ テストの価値を定量評価し、継続的に最適化している

### Integration Tests with External Systems（外部システム統合テスト）

#### 評価軸1: 網羅性（Coverage）

**評価指標**: 自動化率 = 自動テスト済みユースケース数 / 総ユースケース数
- **ユースケース粒度**: 外部連携ユースケース（連携単位） - 外部システムとの1つの連携操作（読み書き、API呼び出し等）

**レベル1: 初期段階**
- □ 自動化された外部システム統合テストが存在しない
- □ 外部連携ユースケースが定義されていない

**レベル2: 管理段階**
- □ ビジネスクリティカルな外部連携ユースケースを一部自動テストしている（自動化率10%以上）
- □ 最も重要なユースケースのみカバー（例: 決済API呼び出し、顧客データDB保存）

**レベル3: 定義段階**
- □ 要件定義またはユーザーストーリーで外部連携ユースケースが定義されている
- □ 定義済みユースケースの50%以上を自動テストしている

**レベル4: 定量管理段階**
- □ 定義済みユースケースの75%以上を自動テストしている

**レベル5: 最適化段階**
- □ 定義済みユースケースの90%以上を自動テストしている
- □ リスクベースのテスト戦略により、ビジネスクリティカルな外部連携に対してより高品質なテストを作成している
- □ 外部システステストメトリクスをダッシュボードで可視化し、チーム全体で共有している
- □ カオスエンジニアリングや障害シナリオテストを実施し、耐障害性を継続的に検証している

---

#### 評価軸2: テストの質（Quality）

**レベル1: 初期段階**
- □ 外部システムテストが存在しない
- □ 本番環境で初めて統合を検証している
- □ テストなしを完了の定義としている
- □ 外部システムのモック/スタブが存在しない

**レベル2: 管理段階**
- □ 正常系のみテスト（障害シナリオは未カバー）
- □ 外部システムのモック/スタブを手動で作成しているが、一貫性がない

**レベル3: 定義段階**
- □ 基本的な障害シナリオ（エラーレスポンス、タイムアウト等）をテストしている
- □ Test Containers、モックサーバー（WireMock等）でテスト環境を構築
- □ テストデータの準備が標準化されている

**レベル4: 定量管理段階**
- □ タイムアウト・リトライ・サーキットブレーカー等の障害シナリオを体系的にテストしている
- □ 主要な外部システムでカオステストを実施している
- □ Service Virtualizationパターンで外部システムを再現
- □ Test Fixture、Database Seedingパターンでデータ準備を自動化

**レベル5: 最適化段階**
- □ ネットワーク障害・データ不整合・パフォーマンス劣化等の包括的な障害シナリオをテストしている
- □ カオステストを定期実行している（週次または月次）
- □ テストパターンが組織的に標準化され、チーム間で共有されている
- □ 不要なテスト削除・重複排除など継続的に最適化している

---

#### 評価軸3: 実行品質（Execution）

**レベル1: 初期段階**
- □ テスト環境が定義されていない
- □ 実行時間を測定していない

**レベル2: 管理段階**
- □ 共有環境でテストを実行している
- □ 実行時間をCIパイプラインで自動測定し、トレンドを記録している
- □ 実行が不安定（環境競合が頻繁に発生）することを認識しているが、対処していない

**レベル3: 定義段階**
- □ チーム専用のテスト環境で実行している
- □ 実行時間が20分未満を維持している
- □ 環境競合が解消され、安定して実行できる

**レベル4: 定量管理段階**
- □ テスト用コンテナ（Docker/Testcontainers等）で開発者ローカルでも実行可能
- □ 実行時間が15分未満を維持している
- □ 実行時間のトレンド分析を行い、劣化を事前に検知している

**レベル5: 最適化段階**
- □ 本番と同等のデータ量・構成の環境でテストを実行している
- □ 実行時間が10分未満を維持している
- □ 実行時間の遅いテストを継続的に最適化している（並列化、テストデータ最適化等）

---

#### 評価軸4: 開発者文化（Culture）

**レベル1: 初期段階**
- □ 開発者がテストを書いていない

**レベル2: 管理段階**
- □ 一部の開発者がテストを書いている

**レベル3: 定義段階**
- □ 開発者がテストを作成・保守している（DORA原則）

**レベル4: 定量管理段階**
- □ 開発者主導のテスト文化が定着している

**レベル5: 最適化段階**
- □ 開発者主導のテスト文化が組織全体に定着している
- □ テストなしの完了の定義（DoD）が組織文化として許容されない

---

#### 評価軸5: 戦略（Strategy）

**レベル1: 初期段階**
- □ テスト戦略が暗黙的（口頭、場当たり的）

**レベル2: 管理段階**
- □ テスト戦略が文書化されている（基本方針レベル：「DB接続とビジネスクリティカルなAPIのみテスト」等）
- □ ビジネスクリティカルな機能のみテスト合格が完了の定義（DoD）の条件

**レベル3: 定義段階**
- □ テスト戦略が詳細に定義されている（外部システム一覧、各システムのテスト方針、障害シナリオカバレッジ等）
- □ 外部システム統合テスト合格が完了の定義（DoD）の前提条件として標準化されている

**レベル4: 定量管理段階**
- □ 障害シナリオをカバーするテスト戦略が文書化されている
- □ すべての外部システムテスト + 障害シナリオテスト合格が完了の定義（DoD）の必須条件

**レベル5: 最適化段階**
- □ カオステスト戦略が文書化され、継続的に最適化されている
- □ テスト戦略を定期的に見直している（四半期ごと等）

### End-to-End Tests（E2Eテスト）

#### 評価軸1: 網羅性（Coverage）

**評価指標**: 自動化率 = 自動テスト済みユースケース数 / 総ユースケース数
- **ユースケース粒度**: エンドツーエンドユースケース（ユーザージャーニー） - ユーザーがシステムを使って達成する一連の業務フロー

**レベル1: 初期段階**
- □ 自動化されたE2Eテストが存在しない（すべて手動）
- □ エンドツーエンドユースケースが定義されていない

**レベル2: 管理段階**
- □ 最頻度のエンドツーエンドユースケース2-3個を自動テストしている
- □ 例: 会員登録→ログイン→商品検索→購入

**レベル3: 定義段階**
- □ 要件定義またはユーザーストーリーでエンドツーエンドユースケースが定義されている
- □ 定義済みユースケースの30%以上を自動テストしている

**レベル4: 定量管理段階**
- □ ほぼすべてのユースケースを自動テストしている（自動化率50%以上）

**レベル5: 最適化段階**
- □ すべての重要ユースケースを自動テストしている（自動化率70%以上、E2Eは100%を目指さない）
- □ リスクベースのテスト戦略により、ユーザー影響の大きいジャーニーに対してより高品質なテストを作成している
- □ E2Eテストメトリクスをダッシュボードで可視化し、チーム全体で共有している
- □ Visual Regression Testingを導入し、UI変更を自動検知している

---

#### 評価軸2: テストの質（Quality）

**レベル1: 初期段階**
- □ E2Eテストが存在しない、または手動テストのみ
- □ リリース前にのみ手動でテストしている
- □ テストなしを完了の定義としている
- □ テストコードがUIの実装詳細に強く依存している（セレクタが脆い）

**レベル2: 管理段階**
- □ 単一ブラウザ（Chrome等）のみでテストしている
- □ 正常系のみテスト（エッジケースは未カバー）
- □ テストコードの重複が多く、保守が困難

**レベル3: 定義段階**
- □ 主要2ブラウザ（Chrome、Safari等）でテストしている
- □ 主要なエッジケースをカバーしている
- □ Page Object Model（POM）パターンでUIロジックを分離
- □ テストがビジネスシナリオに沿って記述されている

**レベル4: 定量管理段階**
- □ 3-4ブラウザ（Chrome、Safari、Firefox、Edge等）でテストしている
- □ すべてのエッジケースをカバーしている
- □ Journey Test、User Flow等の高レベルパターンでテストを整理
- □ Page Objectが再利用可能で、保守性が高い

**レベル5: 最適化段階**
- □ 主要ブラウザすべて + モバイル・タブレットでテストしている
- □ Screenplay Pattern等の高度なパターンで複雑なシナリオを管理
- □ テストパターンが組織的に標準化され、チーム間で共有されている
- □ 不要なテスト削除・重複排除など継続的に最適化している

---

#### 評価軸3: 実行品質（Execution）

**レベル1: 初期段階**
- □ 実行時間を測定していない
- □ Flaky Testの発生率を測定していない

**レベル2: 管理段階**
- □ 実行時間をCIパイプラインで自動測定し、トレンドを記録している
- □ Flaky Test発生を検知しているが、体系的な対処はしていない

**レベル3: 定義段階**
- □ 実行時間が30分未満を維持している（DORA推奨基準）
- □ 実行時間が基準を超えた場合、改善タスクが作成される
- □ Flaky Test率が5%未満
- □ Flaky Testを検知したら自動でIssueが作成される

**レベル4: 定量管理段階**
- □ 実行時間が20分未満を維持している
- □ 実行時間のトレンド分析を行い、劣化を事前に検知している
- □ Flaky Test率が2%未満
- □ Flaky Testの根本原因を分析し、体系的に対処している

**レベル5: 最適化段階**
- □ 実行時間が15分未満を維持している
- □ Flaky Test率が1%未満
- □ 実行時間の遅いテストを継続的に最適化している（並列実行、待機時間削減等）
- □ テスト実行のパフォーマンスメトリクスをダッシュボードで可視化している

---

#### 評価軸4: 開発者文化（Culture）

**レベル1: 初期段階**
- □ 開発者がテストを書いていない

**レベル2: 管理段階**
- □ 一部の開発者がテストを書いている

**レベル3: 定義段階**
- □ 開発者がテストを作成・保守している（DORA原則）

**レベル4: 定量管理段階**
- □ 開発者主導のテスト文化が定着している

**レベル5: 最適化段階**
- □ 開発者主導のテスト文化が組織全体に定着している
- □ テストなしの完了の定義（DoD）が組織文化として許容されない

---

#### 評価軸5: 戦略（Strategy）

**レベル1: 初期段階**
- □ テスト戦略が暗黙的（口頭、場当たり的）

**レベル2: 管理段階**
- □ テスト戦略が文書化されている（基本方針レベル：「最も使われる機能のみ自動化」等）
- □ ビジネスクリティカルな機能のみテスト合格が完了の定義（DoD）の条件

**レベル3: 定義段階**
- □ テスト戦略が詳細に定義されている（ユースケース一覧、各ケースの優先度、ブラウザ対応方針等）
- □ E2Eテスト合格が完了の定義（DoD）の前提条件として標準化されている

**レベル4: 定量管理段階**
- □ クロスブラウザテスト戦略が文書化されている
- □ すべてのE2Eテスト + クロスブラウザテスト合格が完了の定義（DoD）の必須条件

**レベル5: 最適化段階**
- □ テスト戦略を定期的に見直している（四半期ごと等）
- □ テストの価値を定量評価し、継続的に最適化している

---

## ヒートマップ評価方法

### 評価手順

1. **各テストタイプ × 各評価軸を独立して評価**
   - Unit Tests、Component/Integration Tests、External Systems Tests、E2E Testsの4つのテストタイプ
   - 各テストタイプごとに5つの評価軸（網羅性、質、実行品質、開発者文化、戦略）を評価
   - 合計20のセル（4テストタイプ × 5評価軸）を評価

2. **各セルのレベルを判定**
   - 各評価軸の各レベルのチェックリストを確認
   - すべての□にチェックが入る場合のみ、そのレベルに到達
   - 1つでも満たさない項目があれば、そのレベルには未到達
   - 最も高いレベルを記録（レベル1-5）

3. **ヒートマップを作成**
   - 縦軸: 4つのテストタイプ
   - 横軸: 5つの評価軸
   - 各セルに到達レベル（1-5）を記入し、色分け

### ヒートマップの色分け基準

| レベル | 色 | 説明 |
|--------|-----|------|
| **レベル1** | 🔴 赤 | 初期段階 - 早急な改善が必要 |
| **レベル2** | 🟠 オレンジ | 管理段階 - 基本的な取り組みを開始 |
| **レベル3** | 🟡 黄色 | 定義段階 - DORA原則を達成 |
| **レベル4** | 🟢 緑 | 定量管理段階 - データドリブンで最適化 |
| **レベル5** | 🔵 青 | 最適化段階 - 業界トップレベル |

### ヒートマップ評価例

```
                    網羅性  質  実行  文化  戦略
Unit Tests            4    2    5    3    4
Component/IT          3    3    4    3    3
External Systems      2    2    3    2    2
E2E Tests             3    4    2    3    3
```

**ヒートマップ（色分け）:**
```
                    網羅性  質  実行  文化  戦略
Unit Tests            🟢   🟠   🔵   🟡   🟢
Component/IT          🟡   🟡   🟢   🟡   🟡
External Systems      🟠   🟠   🟡   🟠   🟠
E2E Tests             🟡   🟢   🟠   🟡   🟡
```

**この例から読み取れる課題:**
- Unit Testsは「質（レベル2）」が弱点 → アサーションの質、テストの独立性を改善
- External Systemsは全体的に低い → 外部システムテストの基盤整備が必要
- E2E Testsは「実行品質（レベル2）」が弱点 → 実行時間短縮、Flaky Test対策

### 改善の優先順位付け

ヒートマップを使って、以下の観点で優先順位を決定します:

1. **赤いセル（レベル1）を最優先で改善**
   - 最も重要な基盤が欠けている状態

2. **テストピラミッドの下層から改善**
   - Unit Tests → Component/Integration Tests → External Systems Tests → E2E Tests
   - 下層が安定していないと、上層の品質が保てない

3. **開発者文化と戦略を早期に改善**
   - 他の評価軸（網羅性、質、実行品質）の改善には、開発者文化と戦略が土台として必要

4. **バランスの取れた改善**
   - 1つの評価軸だけレベル5にしても、他がレベル1-2では効果が限定的
   - 全体をレベル3に引き上げることを最初の目標にする

### 定期的な見直し

- **評価頻度**: 四半期ごとに再評価
- **トレンド分析**: 前回評価との比較で改善を可視化
- **目標設定**: 次の四半期で改善すべきセルを特定

---

**最終更新**: 2025-11-18
**バージョン**: 19.0 (AI関連の記述を削除: 実践的な成熟度評価に焦点)
